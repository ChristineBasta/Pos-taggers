{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import string\n",
    "\n",
    "from nltk.tag import tnt\n",
    "from itertools import chain\n",
    "import nltk\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "from sklearn.preprocessing import LabelBinarizer\n",
    "import sklearn\n",
    "\n",
    "from sklearn_crfsuite import scorers, metrics, CRF\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_train_from_klc():\n",
    "    \"\"\"\n",
    "        Extracts training data from KLC dataset. \n",
    "        Returns list of sentences. \n",
    "        Every sentence is list of tuples where fist element is word, second is tag\n",
    "    \"\"\"\n",
    "    data = []\n",
    "    with open('klc/data1.xml') as f:\n",
    "        sen = []\n",
    "        w, base = None, None\n",
    "        for line in f:\n",
    "            if line.startswith('<w>'):\n",
    "                w = line[3:-6]\n",
    "            elif line.startswith('<base>'):\n",
    "                base = line[6:-9]\n",
    "                sen.append((w, base))\n",
    "            elif line.startswith('</tokens>'):\n",
    "                data.append(sen)\n",
    "                sen = []\n",
    "\n",
    "    return data\n",
    "\n",
    "\n",
    "def extract_test_from_klc():\n",
    "    \"\"\"\n",
    "        Extracts testing data from KLC dataset. \n",
    "        Returns list of sentences. \n",
    "        Every sentence is list of tuples where fist element is word, second is tag\n",
    "    \"\"\"\n",
    "    data = []\n",
    "    with open('klc/data2.xml') as f:\n",
    "        sen = []\n",
    "        w, base = None, None\n",
    "        for line in f:\n",
    "            if line.startswith('<w>'):\n",
    "                w = line[3:-6]\n",
    "            elif line.startswith('<base>'):\n",
    "                base = line[6:-9]\n",
    "                sen.append((w, base))\n",
    "            elif line.startswith('</tokens>'):\n",
    "                data.append(sen)\n",
    "                sen = []\n",
    "\n",
    "    return data\n",
    "\n",
    "\n",
    "def get_data(data_file=\"train\"):\n",
    "    \"\"\"\n",
    "        Extracts training or testing data from KLC dataset. \n",
    "        Returns list of sentences. \n",
    "        Every sentence is list of tuples where fist element is word, second is tag\n",
    "    \"\"\"\n",
    "    data = []\n",
    "    with open('UD_Kazakh/kk-{}.conllu'.format(data_file)) as f:\n",
    "        sen = []\n",
    "        for line in f:\n",
    "            if not line.strip():\n",
    "                data.append(sen)\n",
    "                sen = []\n",
    "            if line[0] in string.digits:\n",
    "                splitted = line.split()\n",
    "                if splitted[3] == \"PUNCT\":\n",
    "                    sen.append((splitted[1], splitted[1]))\n",
    "                else:\n",
    "                    sen.append((splitted[1], splitted[3]))\n",
    "    return data\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "calculate accuracy for TnT tagger on UD dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.4557124518613607"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tagger = tnt.TnT(N=100000)\n",
    "\n",
    "train_ud = get_data()\n",
    "test_ud = get_data(data_file=\"test\")\n",
    "\n",
    "tagger.train(train_ud)\n",
    "tagger.evaluate(test_ud)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "calculate accuracy for TnT on KLC dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.17421602787456447"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_klc = extract_train_from_klc()\n",
    "test_klc = extract_test_from_klc()\n",
    "tagger.train(train_klc)\n",
    "tagger.evaluate(test_klc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "calculating accuracy of CRF on UD dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "UD_pos_tags = set(['ADJ', 'ADP', 'ADV', 'AUX', 'CCONJ', 'DET', 'INTJ', 'NOUN', 'NUM', 'PART', 'PRON', 'PROPN', 'PUNCT', 'SCONJ', 'SYM', 'VERB', 'X'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def word2features(sent, pos):\n",
    "    \"\"\"\n",
    "        function takes sentence and extracts features for word on `pos` position\n",
    "    \"\"\"\n",
    "    word = sent[pos]\n",
    "    features = {\n",
    "        'bias': 1.0,\n",
    "        'word.lower': word.lower(),\n",
    "        'word[-3:]': word[-3:],\n",
    "        'word[-2:]': word[-2:],\n",
    "        'word.isupper': word.isupper(),\n",
    "        'word.istitle': word.istitle(),\n",
    "        'word.isdigit': word.isdigit(),\n",
    "    }\n",
    "                \n",
    "    if pos > 0:\n",
    "        word1 = sent[pos-1]\n",
    "        features.update({\n",
    "            '-1:word.lower()': word1.lower(),\n",
    "            '-1:word.istitle()': word1.istitle(),\n",
    "            '-1:word.isupper()': word1.isupper(),\n",
    "        })\n",
    "    else:\n",
    "        features['BOS'] = True\n",
    "\n",
    "    if pos < len(sent)-1:\n",
    "        word1 = sent[pos+1]\n",
    "        features.update({\n",
    "            '+1:word.lower()': word1.lower(),\n",
    "            '+1:word.istitle()': word1.istitle(),\n",
    "            '+1:word.isupper()': word1.isupper(),\n",
    "        })\n",
    "    else:\n",
    "        features['EOS'] = True    \n",
    "    return features\n",
    "\n",
    "def sent2features(sent):\n",
    "    \"\"\"\n",
    "        function takes sentence and converts each word to set of features\n",
    "    \"\"\"\n",
    "    return [word2features(sent, i) for i in range(len(sent))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 80 ms, sys: 4 ms, total: 84 ms\n",
      "Wall time: 81.3 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "import string\n",
    "\n",
    "X_train, X_test, y_train, y_test = [], [], [], []\n",
    "x, y = [], []\n",
    "\n",
    "# building training set\n",
    "with open('UD_Kazakh/kk-train.conllu') as f:\n",
    "    for line in f:\n",
    "        if not line.strip():\n",
    "            X_train.append(sent2features(x))\n",
    "            y_train.append(y)\n",
    "            x, y = [], []\n",
    "        \n",
    "        if line[0] in string.digits:\n",
    "            splitted = line.split()\n",
    "            \n",
    "            if splitted[3] == '_':\n",
    "                continue\n",
    "            \n",
    "            if splitted[3] != \"PUNCT\":\n",
    "                x.append(splitted[1])\n",
    "                y.append(splitted[3])\n",
    "            else:\n",
    "                x.append(splitted[1])\n",
    "                y.append(splitted[1])\n",
    "\n",
    "# building test set\n",
    "with open('UD_Kazakh/kk-test.conllu') as f:\n",
    "    for line in f:\n",
    "        if not line.strip():\n",
    "            X_test.append(sent2features(x))\n",
    "            y_test.append(y)\n",
    "            x, y = [], []\n",
    "        \n",
    "        if line[0] in string.digits:\n",
    "            splitted = line.split()\n",
    "            \n",
    "            if splitted[3] == '_':\n",
    "                continue\n",
    "            if splitted[3] != \"PUNCT\":\n",
    "                x.append(splitted[1])\n",
    "                y.append(splitted[3])\n",
    "            else:\n",
    "                x.append(splitted[1])\n",
    "                y.append(splitted[1])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 9.92 s, sys: 0 ns, total: 9.92 s\n",
      "Wall time: 9.91 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "crf = CRF(\n",
    "    algorithm='lbfgs',\n",
    "    c1=0.1,\n",
    "    c2=0.1,\n",
    "    max_iterations=100,\n",
    "    all_possible_transitions=True\n",
    ")\n",
    "crf.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = [cls for cls in crf.classes_ if cls in string.punctuation or cls in UD_pos_tags]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/tamirok/anaconda3/envs/py27/lib/python2.7/site-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "/home/tamirok/anaconda3/envs/py27/lib/python2.7/site-packages/sklearn/metrics/classification.py:1137: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no true samples.\n",
      "  'recall', 'true', average, warn_for)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.65851737034768043"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred = crf.predict(X_test)\n",
    "\n",
    "metrics.flat_f1_score(y_test, y_pred, average='weighted', labels=labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             precision    recall  f1-score   support\n",
      "\n",
      "      PROPN      0.714     0.119     0.204        42\n",
      "        NUM      0.739     0.773     0.756        22\n",
      "       NOUN      0.603     0.850     0.705       200\n",
      "        AUX      0.750     0.533     0.623        45\n",
      "          .      0.961     1.000     0.980        49\n",
      "       VERB      0.618     0.711     0.661       114\n",
      "          ,      1.000     1.000     1.000        57\n",
      "       PRON      0.833     0.484     0.612        31\n",
      "          ?      1.000     1.000     1.000         3\n",
      "        DET      0.429     0.545     0.480        11\n",
      "        ADJ      0.528     0.418     0.467        67\n",
      "          -      1.000     1.000     1.000         4\n",
      "        ADP      0.818     0.692     0.750        13\n",
      "        ADV      0.545     0.522     0.533        23\n",
      "          X      0.200     0.111     0.143         9\n",
      "      CCONJ      0.333     0.375     0.353         8\n",
      "          (      1.000     1.000     1.000         5\n",
      "          )      1.000     1.000     1.000         5\n",
      "          !      1.000     1.000     1.000         4\n",
      "      SCONJ      0.000     0.000     0.000         8\n",
      "       INTJ      0.000     0.000     0.000         0\n",
      "        SYM      0.000     0.000     0.000         1\n",
      "          \"      0.000     0.000     0.000         0\n",
      "       PART      0.000     0.000     0.000         6\n",
      "          :      1.000     1.000     1.000         4\n",
      "          _      0.000     0.000     0.000         0\n",
      "          ;      0.000     0.000     0.000         0\n",
      "\n",
      "avg / total      0.675     0.687     0.659       731\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/tamirok/anaconda3/envs/py27/lib/python2.7/site-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "/home/tamirok/anaconda3/envs/py27/lib/python2.7/site-packages/sklearn/metrics/classification.py:1137: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples.\n",
      "  'recall', 'true', average, warn_for)\n"
     ]
    }
   ],
   "source": [
    "print(metrics.flat_classification_report(y_test, y_pred, digits=3, labels=labels))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "calculating accuracy of CRF tagger on KLC dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_klc, X_test_klc, y_train_klc, y_test_klc = [], [], [], []\n",
    "\n",
    "for sen in train_klc:\n",
    "    y_train_klc.append([])\n",
    "    x_train = []\n",
    "    for word, tag in sen:\n",
    "        x_train.append(word)\n",
    "        y_train_klc[-1].append(tag)\n",
    "    X_train_klc.append(sent2features(x_train))\n",
    "\n",
    "for sen in test_klc:\n",
    "    y_test_klc.append([])\n",
    "    x_test = []\n",
    "    for word, tag in sen:\n",
    "        x_test.append(word)\n",
    "        y_test_klc[-1].append(tag)\n",
    "    X_test_klc.append(sent2features(x_test))\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 460 ms, sys: 12 ms, total: 472 ms\n",
      "Wall time: 467 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "crf = CRF(\n",
    "    algorithm='lbfgs',\n",
    "    c1=0.1,\n",
    "    c2=0.1,\n",
    "    max_iterations=100,\n",
    "    all_possible_transitions=True\n",
    ")\n",
    "crf.fit(X_train_klc, y_train_klc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_klc = crf.predict(X_test_klc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(['SIMU', 'ZEP', 'ZEP', 'ZEP', 'ZEP', 'ET', '.'],\n",
       " ['SIMU', 'ZEP', 'ZEP', 'ESM', 'ZEP', 'US', '.'])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred_klc[0], y_test_klc[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.55704292118257637"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "metrics.flat_f1_score(y_test_klc, y_pred_klc, average='weighted')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
