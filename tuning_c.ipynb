{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from subprocess import check_call\n",
    "\n",
    "cluster_sizes = [150, 300, 600, 800, 1000, 1500, 2000, 2500, 3000]\n",
    "\n",
    "# running brown clustering on test_words(UD dataset)\n",
    "for cluster_size in cluster_sizes:\n",
    "    check_call(['brown-cluster/wcluster', '--text', 'test_words.txt', '--c', str(cluster_size), '--threads', '4'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "from sklearn.preprocessing import LabelBinarizer\n",
    "import sklearn\n",
    "\n",
    "from sklearn_crfsuite import scorers, metrics, CRF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(150, 0.61556308140159333)\n",
      "(300, 0.55565219534847932)\n",
      "(600, 0.57907536073365085)\n",
      "(800, 0.5843288523740503)\n",
      "(1000, 0.53591663318130511)\n",
      "(1500, 0.55070383168783266)\n",
      "(2000, 0.52841687438742579)\n",
      "(2500, 0.5620465969818873)\n",
      "(3000, 0.57546510991673172)\n"
     ]
    }
   ],
   "source": [
    "UD_pos_tags = set(['ADJ', 'ADP', 'ADV', 'AUX', 'CCONJ', 'DET', 'INTJ', 'NOUN', 'NUM', 'PART', 'PRON', 'PROPN', 'PUNCT', 'SCONJ', 'SYM', 'VERB', 'X'])\n",
    "\n",
    "def word2features(sent, pos, path_dict):\n",
    "    \"\"\"\n",
    "        function takes sentence and extracts features for word on `pos` position\n",
    "    \"\"\"        \n",
    "    \n",
    "    word = sent[pos]\n",
    "    features = {\n",
    "        'bias': 1.0,\n",
    "        'word.lower': word.lower(),\n",
    "        'word[-3:]': word[-3:],\n",
    "        'word[-2:]': word[-2:],\n",
    "        'word.isupper': word.isupper(),\n",
    "        'word.istitle': word.istitle(),\n",
    "        'word.isdigit': word.isdigit(),\n",
    "    }\n",
    "    # use prefixes of length 2,4,8... as feature for token `sent[pos]`   \n",
    "    for length in range(2, min(17, len(path_dict[word])), 2):\n",
    "        features['path_pref_{}'.format(length)] = path_dict[word][:length]\n",
    "        \n",
    "    if pos > 0:\n",
    "        word1 = sent[pos-1]\n",
    "        features.update({\n",
    "            '-1:word.lower()': word1.lower(),\n",
    "            '-1:word.istitle()': word1.istitle(),\n",
    "            '-1:word.isupper()': word1.isupper(),\n",
    "        })\n",
    "        \n",
    "         # use prefixes of length 2,4,8... as feature for next token `sent[pos+1]` \n",
    "        for length in range(2, min(17, len(path_dict[word1])), 2):\n",
    "            features['-1:path_pref_{}'.format(length)] = path_dict[word1][:length]\n",
    "        \n",
    "    else:\n",
    "        features['BOS'] = True\n",
    "\n",
    "    if pos < len(sent)-1:\n",
    "        word1 = sent[pos+1]\n",
    "        features.update({\n",
    "            '+1:word.lower()': word1.lower(),\n",
    "            '+1:word.istitle()': word1.istitle(),\n",
    "            '+1:word.isupper()': word1.isupper(),\n",
    "        })\n",
    "        \n",
    "        # use prefixes of length 2,4,8... as feature for previous token `sent[pos-1]`\n",
    "        for length in range(2, min(17, len(path_dict[word1])), 2):\n",
    "            features['+1:path_pref_{}'.format(length)] = path_dict[word1][:length]\n",
    "        \n",
    "    else:\n",
    "        features['EOS'] = True    \n",
    "        \n",
    "    return features\n",
    "\n",
    "def sent2features(sent, path_dict):\n",
    "    \"\"\"\n",
    "        function takes sentence and converts each word to set of features\n",
    "    \"\"\"\n",
    "    return [word2features(sent, i, path_dict) for i in range(len(sent))]\n",
    "\n",
    "\n",
    "import string\n",
    "\n",
    "for cluster_size in cluster_sizes:\n",
    "    \n",
    "    path_dict, path_test_dict = {}, {}\n",
    "    \n",
    "    with open('train_words-c{}-p1.out/paths'.format(cluster_size)) as f:\n",
    "        for line in f:\n",
    "            splitted = line.split()\n",
    "            path_dict[splitted[1]] = splitted[0]\n",
    "    \n",
    "    \n",
    "    with open('test_words-c{}-p1.out/paths'.format(cluster_size)) as f:\n",
    "        for line in f:\n",
    "            splitted = line.split()\n",
    "            path_test_dict[splitted[1]] = splitted[0]\n",
    "    \n",
    "    \n",
    "    X_train, X_test, y_train, y_test = [], [], [], []\n",
    "    x, y = [], []\n",
    "\n",
    "    distinct = set()\n",
    "    corpus_size = 0\n",
    "\n",
    "    # building training set\n",
    "    with open('UD_Kazakh/kk-train.conllu') as f:\n",
    "        for line in f:\n",
    "            if not line.strip():\n",
    "                X_train.append(sent2features(x, path_dict))\n",
    "                y_train.append(y)\n",
    "                x, y = [], []\n",
    "\n",
    "            if line[0] in string.digits:\n",
    "                splitted = line.split()\n",
    "\n",
    "                if splitted[3] == '_':\n",
    "                    continue\n",
    "\n",
    "                if splitted[3] != \"PUNCT\":\n",
    "                    x.append(splitted[1])\n",
    "                    y.append(splitted[3])\n",
    "                else:\n",
    "                    x.append(splitted[1])\n",
    "                    y.append(splitted[1])\n",
    "\n",
    "                corpus_size += 1\n",
    "                distinct.add(splitted[1])\n",
    "\n",
    "\n",
    "    # building test set\n",
    "    with open('UD_Kazakh/kk-test.conllu') as f:\n",
    "        for line in f:\n",
    "            if not line.strip():\n",
    "                X_test.append(sent2features(x, path_test_dict))\n",
    "                y_test.append(y)\n",
    "                x, y = [], []\n",
    "                \n",
    "                \n",
    "            if line[0] in string.digits:\n",
    "                splitted = line.split()\n",
    "\n",
    "                if splitted[3] == '_':\n",
    "                    continue\n",
    "                if splitted[3] != \"PUNCT\":\n",
    "                    x.append(splitted[1])\n",
    "                    y.append(splitted[3])\n",
    "                else:\n",
    "                    x.append(splitted[1])\n",
    "                    y.append(splitted[1])\n",
    "\n",
    "\n",
    "    crf = CRF(\n",
    "        algorithm='lbfgs',\n",
    "        c1=0.1,\n",
    "        c2=0.1,\n",
    "        max_iterations=100,\n",
    "        all_possible_transitions=True\n",
    "    )\n",
    "\n",
    "    crf.fit(X_train, y_train)\n",
    "    y_pred = crf.predict(X_test)\n",
    "    labels = [cls for cls in crf.classes_ if cls in string.punctuation or cls in UD_pos_tags]\n",
    "    print(cluster_size, metrics.flat_precision_score(y_test, y_pred, average='weighted', labels=labels))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
